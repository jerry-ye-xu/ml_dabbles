{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"//fonts.googleapis.com/css?family=Quicksand:300\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"//fonts.googleapis.com/css?family=Quicksand:300\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning basics\n",
    "\n",
    "- Rodrigo Agundez\n",
    "- Amsterdam @ Booking.com\n",
    "- Friday May 25th, 2018\n",
    "\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning over the years\n",
    "\n",
    "![three_quarters center](images/dl_timeline.png)\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning over the years\n",
    "\n",
    "- 1958 - Percentron unit - Frank Rosenblatt\n",
    "- **1986 - Backpropagation - Geoffrey Hinton**\n",
    "- 1986 - RNN - Schuster & Pallwal\n",
    "- 1989 - LeNet Backpropagation to multi-layer perceptron - Yan LeCun\n",
    "- 1997 - LSTM - Sepp Hochreiter and Jürgen Schmidhuber\n",
    "- **1998 - LeNet-5 Convolutional neural networks - YanLecun**\n",
    "- 2007 - Fei Fei Li Princeton ImageNet competition\n",
    "- 2009 - GPU for deep learning - Andrew Ng\n",
    "- **2011 - Demonstration of ReLu for deep neural networks - Yoshua Bengio**\n",
    "- **2012 - AlexNet wins ImageNet 25% to 16% error**\n",
    "- 2012 - Dropout technique - Geoffrey Hinton\n",
    "- **2014 - Generative adversarial networks - Ian Goodfellow & Yoshua Bengio**\n",
    "- **2015 - CNN beats human error in ImageNet 5% to 3%**\n",
    "- **2016 - AlphaGo - Goole DeepMind**\n",
    "- 2016 - Detectic metatastic cancer beats human pathologist .96 to 0.99 AUC\n",
    "- 2017 - Capsule networks - Geoffrey Hinton\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning over the years\n",
    "\n",
    "![three_quarters center](images/lecunn_quote.jpg)\n",
    "\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "\n",
    "![left half](images/neurons.jpg)\n",
    "![right half](images/dnn.png)\n",
    "\n",
    "&nbsp;\n",
    "<br><br>\n",
    "<sub>*[medium.com/@xenonstack](https://medium.com/@xenonstack/log-analytics-with-deep-learning-and-machine-learning-20a1891ff70e)*<sub/>\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neuron description\n",
    "\n",
    "![neuron_comparison center half](images/neuron_comparison.png)\n",
    "\n",
    "Frist artificial neuron proposed in 1943!\n",
    "\n",
    "\n",
    "<sub>*https://appliedgo.net/perceptron*</sub>\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Neural Networks - Components\n",
    "\n",
    "![dnn center half](images/dnn_labels.png)\n",
    "\n",
    "<sub>*[medium.com/@xenonstack](https://medium.com/@xenonstack/log-analytics-with-deep-learning-and-machine-learning-20a1891ff70e)*<sub/>\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Artificial neuron\n",
    "\n",
    "![artificial_neuron center half](images/artificial_neuron.jpg)\n",
    "\n",
    "\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Layers\n",
    "\n",
    "Hierarchical feature representations\n",
    "\n",
    "![layers center](images/layers.jpg)\n",
    "\n",
    "<sub>*[leonardoaraujosantos.gitbooks.io](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/neural_networks.html)*</sub>\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![neural_networks_collection](images/neural_networks_collection.png)\n",
    "\n",
    "<sub>*[leonardoaraujosantos.gitbooks.io](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/neural_networks.html)*</sub>\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Typical training flow\n",
    "\n",
    "\n",
    "![center half](images/model_diagram.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass\n",
    "\n",
    "![center half](images/forward_pass_0.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass\n",
    "\n",
    "\n",
    "![center half](images/forward_pass_1.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass\n",
    "\n",
    "\n",
    "![center half](images/forward_pass_2.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "\n",
    "\n",
    "![center half](images/backpropagation_0.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "\n",
    "\n",
    "![center half](images/backpropagation_1.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "\n",
    "\n",
    "![center half](images/backpropagation_2.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "\n",
    "\n",
    "![center half](images/backpropagation_3.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "\n",
    "\n",
    "Starts at the end of the net and tunes each layer using the gradient of the loss function. Repeatedly applies the chain rule. \n",
    "\n",
    "**Numeric approximation** $f'(x) \\approx \\frac{f(x+h) - f(x)}{h}$\n",
    "\n",
    "**Symbolic differentiation** Symbolic, exact representation of the derivative.\n",
    "\n",
    "**Reverse automatic differentiation**\n",
    "\n",
    "![center](images/automatic-differentiation.png)\n",
    "\n",
    "<sup>Source: [Automatic Differentiation](http://www.columbia.edu/~ahd2125/post/2015/12/5/)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Optimization by backpropagation\n",
    "\n",
    "1. Loss/cost function\n",
    "2. Gradient descent\n",
    " 1. Stochastic gradient descent (SGD)\n",
    " 2. SGD + momentum\n",
    " 3. SGD + Nesterov\n",
    " 4. AdaGrad\n",
    " 5. RMSProp\n",
    " 6. Adam\n",
    " 7. Nadam\n",
    "\n",
    "<sub>*http://ruder.io/optimizing-gradient-descent/*</sub>\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Optimizers\n",
    "\n",
    "![optimizers_1 left half](images/optimizers_1.gif)\n",
    "![optimizers_2 right half](images/optimizers_2.gif)\n",
    "\n",
    "<sub>*http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html*</sub>\n",
    "<sub>*([Alec Radford](https://twitter.com/alecrad))*<sub/>\n",
    "\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When to use neural nets?\n",
    "\n",
    "** Data types**\n",
    "\n",
    "- **Structured data**\n",
    "    - Handcrafted feature engineering\n",
    "    - Boosting algoritms\n",
    "\n",
    "- **Unstructured data** \n",
    "    - (images/text/signals) use neural networks and deep learning\n",
    "\n",
    "<sup>[How to win a kaggle competition](https://www.import.io/post/how-to-win-a-kaggle-competition/) by Kaggle CEO and Founder, Anthony Goldbloom</sup>\n",
    "\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# When to use neural nets?\n",
    "\n",
    "**Dataset size**\n",
    "\n",
    "To cite the [Deep Learning](http://www.deeplearningbook.org/contents/intro.html) book:\n",
    "\n",
    ">  As of 2016, a rough rule of thumb is that a supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category, and will match or exceed human performance when trained with a dataset containing at least 10 million labeled examples. Working successfully with datasets smaller than this is an important research area, focusing in particular on how we can take advantage of large quantities of unlabeled examples, with unsupervised or semi-supervised learning.\n",
    "\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recommended literature\n",
    "\n",
    "![center quarter](images/dl_book.jpg)\n",
    "\n",
    "![footer_logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recommended training\n",
    "\n",
    "![center quarter](images/logo.png)\n",
    "<p style=\"text-align:center\">\n",
    "Trainer: Rodrigo Agundez\n",
    "</p>\n",
    "\n",
    "||           \t|                      \t| \n",
    "|------|-----------\t|----------------------\t|\n",
    "|[Dutch Data Science Week: training special](https://www.eventbrite.nl/e/tickets-training-special-deep-learning-dutch-data-science-week-2018-44832464107)| May           | 29         | \n",
    "|[GoDataDriven 3-days Deep Learning](https://training.xebia.com/data-science/deep-learning/?__hstc=153823591.74dad2ca052e57b296078a5c290a2612.1526800565238.1526996056463.1527177906681.5&__hssc=153823591.1.1527177906681&__hsfp=4099784312) | November      \t| 21-23 \t|\n",
    "\n",
    "- training@xebia.com\n",
    "- thomasdeboer@godatadriven.com\n",
    "- https://godatadriven.com/training-overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Next: Deep learning tools](02_deep_learning_tools.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
